{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from create_batch import mini_batch\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import enjoy_tree\n",
    "tf.set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sample_data.bin', 'rb') as f:\n",
    "    batches_morphene = pickle.load(f)\n",
    "    batches_arc = pickle.load(f)\n",
    "    batches_relation = pickle.load(f)\n",
    "    dictionary_tag = pickle.load(f)\n",
    "    dictionary_rel = pickle.load(f)\n",
    "    dictionary_char = pickle.load(f)\n",
    "    label_dim = pickle.load(f)\n",
    "with open('gensim_embedding_100.txt', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "def get_vector(word):\n",
    "    vector = model.wv[word]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dim = 100\n",
    "char_dim = len(dictionary_char)\n",
    "word_max_length = 32\n",
    "word_vec_dim = 100\n",
    "char_vec_dim = 100\n",
    "concat_dim = 200\n",
    "input_dim = 100\n",
    "sequence_length = 220\n",
    "in_size = concat_dim\n",
    "hidden_size = 200\n",
    "arc_size = 400\n",
    "rel_size = 100\n",
    "dense_size = 64\n",
    "regularization_scale = 0.001\n",
    "beta = 0.009\n",
    "epsilon = 2e-3\n",
    "grad_clip = 5\n",
    "dropout_rate = 0.33\n",
    "learning_rate = 0.002\n",
    "batch_size = 1\n",
    "unroll_step = sequence_length\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=regularization_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs_vec, train_inputs_tag, train_inputs_char, train_labels_arc, train_labels_rel = [], [], [], [], []\n",
    "training = tf.placeholder(dtype=tf.bool, name=\"training\")\n",
    "for ui in range(unroll_step):\n",
    "    train_inputs_vec.append(tf.placeholder(tf.float32, shape=[None, input_dim], name='train_inputs_vec_%d'%ui))\n",
    "    train_inputs_tag.append(tf.placeholder(tf.float32, shape=[None, input_dim], name='train_inputs_tag_%d'%ui))\n",
    "    train_inputs_char.append(tf.placeholder(tf.float32, shape=[None, word_max_length, char_dim, 1], name='train_inputs_char_%d'%ui))\n",
    "prev_train_h_fd = tf.Variable(tf.zeros([batch_size, hidden_size],dtype=tf.float32),name='train_h_fd',trainable=False)\n",
    "prev_train_h_bd = tf.Variable(tf.zeros([batch_size, hidden_size],dtype=tf.float32),name='train_h_bd',trainable=False)\n",
    "train_labels_arc = tf.placeholder(tf.float32, shape=[batch_size, sequence_length, sequence_length], name='train_labels_arc')\n",
    "train_labels_rel = tf.placeholder(tf.float32, shape=[batch_size, sequence_length, label_dim], name='train_labels_rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('a'):\n",
    "    w_forward = tf.get_variable(name=\"w_forward\", shape=[3,in_size, hidden_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    u_forward = tf.get_variable(name=\"u_forward\", shape=[3,hidden_size, hidden_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_forward = tf.get_variable(name=\"b_forward\", shape=[3,hidden_size], initializer=tf.zeros_initializer())\n",
    "    w_backward = tf.get_variable(name=\"w_backward\", shape=[3,in_size, hidden_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    u_backward = tf.get_variable(name=\"u_backward\", shape=[3,hidden_size, hidden_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_backward = tf.get_variable(name=\"b_backward\", shape=[3,hidden_size], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('b'):\n",
    "    w_arc_dep = tf.get_variable(name=\"w_arc_dep\", shape=[hidden_size, arc_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_arc_dep = tf.get_variable(name=\"b_arc_dep\", shape=[arc_size], initializer=tf.zeros_initializer())\n",
    "    w_arc_head = tf.get_variable(name=\"w_arc_head\", shape=[hidden_size, arc_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_arc_head = tf.get_variable(name=\"b_arc_head\", shape=[arc_size], initializer=tf.zeros_initializer())\n",
    "    w_rel_dep = tf.get_variable(name=\"w_rel_dep\", shape=[hidden_size, rel_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_rel_dep = tf.get_variable(name=\"b_rel_dep\", shape=[rel_size], initializer=tf.zeros_initializer())\n",
    "    w_rel_head = tf.get_variable(name=\"w_rel_head\", shape=[hidden_size, rel_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_rel_head = tf.get_variable(name=\"b_rel_head\", shape=[rel_size], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('biaffine_classifier'):\n",
    "    w_arc = tf.get_variable(name=\"w_arc\", shape=[arc_size, arc_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_arc = tf.get_variable(name=\"b_arc\", shape=[arc_size, 1], regularizer=regularizer)\n",
    "    w_rel = tf.get_variable(name=\"w_rel\", shape=[label_dim, concat_dim], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    u_rel = tf.get_variable(name=\"u_rel\", shape=[rel_size, rel_size], initializer=tf.contrib.layers.xavier_initializer(), regularizer=regularizer)\n",
    "    b_rel = tf.get_variable(name=\"b_rel\", shape=[label_dim, 1], regularizer=regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSUNG_LIST = ['ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ']\n",
    "\n",
    "JUNGSUNG_LIST = ['ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ','ᅵ']\n",
    "\n",
    "JONGSUNG_LIST = [' ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ','ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
    "\n",
    "INDI_LIST = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ',\n",
    "             'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ',\n",
    "             'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "hangul_johab = range(44032,55204)\n",
    "hangul_jaeum = range(12593,12623)\n",
    "hangul_moeum = range(12623,12644)\n",
    "hangul_chosung = range(4352,4371)\n",
    "hangul_jungsung = range(4449,4470)\n",
    "hangul_jongsung = range(4520,4547)\n",
    "english1 = range(65,91)\n",
    "english2 = range(97,123)\n",
    "digit = range(48,58)\n",
    "special_char = [ord('.'), ord('\\''), ord('?'), ord(','), ord('!'), ord('%')]\n",
    "\n",
    "def syllable(char):\n",
    "    s = ord(char) - 44032\n",
    "    cho = (s//21)//28\n",
    "    jung = (s%(21*28))//28\n",
    "    jong = (s%28)\n",
    "    \n",
    "    return CHOSUNG_LIST[cho], JUNGSUNG_LIST[jung], JONGSUNG_LIST[jong]\n",
    "\n",
    "def split_data(batches):\n",
    "    batches_word_vector = []\n",
    "    batches_character_jaso = []\n",
    "    batches_tag = []\n",
    "    batches_vector_append = batches_word_vector.append\n",
    "    batches_jaso_append = batches_character_jaso.append\n",
    "    batches_tag_append = batches_tag.append\n",
    "    UNK_TOKEN = np.zeros(word_vec_dim, np.float32)\n",
    "    PAD_VECTOR = np.zeros(word_vec_dim, np.float32)\n",
    "    JASO_PAD = np.zeros(word_max_length, np.float32)\n",
    "    for line in batches:\n",
    "        batch_jaso = []\n",
    "        batch_wordvector = []\n",
    "        batch_tag = []\n",
    "        t_append = batch_tag.append\n",
    "        v_append = batch_wordvector.append\n",
    "\n",
    "        for w_ in line:\n",
    "            v = re.sub(r\"/{1}([A-Z]+)\", r\" \\1\", w_)\n",
    "            v = v.split(' ')\n",
    "            if v[0] in model.wv.vocab:\n",
    "                v_append(get_vector(v[0]))\n",
    "            else:\n",
    "                v_append(UNK_TOKEN)\n",
    "            t_append(v[-1])\n",
    "            word = []\n",
    "            w_append = word.append\n",
    "            w_extend = word.extend\n",
    "            for c in v[0]:\n",
    "                    sign_unk = 0\n",
    "                    if ord(c) in hangul_johab or ord(c) in hangul_chosung or \\\n",
    "                       ord(c) in hangul_jungsung or ord(c) in hangul_jongsung or \\\n",
    "                       ord(c) in hangul_jaeum or ord(c) in hangul_moeum or \\\n",
    "                       ord(c) in english1 or ord(c) in english2 or \\\n",
    "                       ord(c) in digit or ord(c) in special_char: pass\n",
    "                    else: sign_unk = 1\n",
    "\n",
    "                    if sign_unk == 1:\n",
    "                        w_append('<UNK>')\n",
    "                    else:\n",
    "                        if ord(c) in hangul_johab:\n",
    "                            jaso = syllable(c)\n",
    "                            if ' ' in jaso:\n",
    "                                jaso = jaso[0:-1]\n",
    "                            w_extend(jaso)\n",
    "                        else:\n",
    "                            w_append(c)\n",
    "            batch_jaso.append(word)\n",
    "        batches_jaso_append(batch_jaso)\n",
    "        batches_tag_append(batch_tag)\n",
    "        batches_vector_append(batch_wordvector)\n",
    "\n",
    "    for i in range(len(batches_word_vector)):\n",
    "        present_length = len(batches_word_vector[i])\n",
    "        for _ in range(present_length, sequence_length):\n",
    "            batches_word_vector[i].append(PAD_VECTOR)\n",
    "\n",
    "    indexed_batches = []\n",
    "    batches_jaso = []\n",
    "    d_append = indexed_batches.append\n",
    "    for sentence in batches_character_jaso:\n",
    "        sen = []\n",
    "        s_append = sen.append\n",
    "        for word in sentence:\n",
    "            s_append(([dictionary_char[char] for char in word]))\n",
    "        d_append(sen)\n",
    "\n",
    "    for i in range(len(indexed_batches)):\n",
    "        sequence = []\n",
    "        sentence_length = len(indexed_batches[i])\n",
    "        for j in range(sentence_length):\n",
    "            present_length = len(indexed_batches[i][j])\n",
    "            padding_length = word_max_length - present_length\n",
    "            if(padding_length % 2 == 0):\n",
    "                left = right = padding_length // 2\n",
    "            else:\n",
    "                right = padding_length // 2\n",
    "                left = right + 1\n",
    "            sequence.append(np.pad(indexed_batches[i][j], (left, right), 'constant', constant_values=0))\n",
    "        for k in range(sentence_length, sequence_length):\n",
    "            sequence.append(JASO_PAD)\n",
    "        batches_jaso.append(sequence)\n",
    "\n",
    "    batches_jaso_input = np.zeros([len(batches), sequence_length, word_max_length, char_dim])\n",
    "    for batch_index in range(len(batches_jaso)):\n",
    "        for sequence_index in range(len(batches_jaso[batch_index])):\n",
    "            for word_index in range(len(batches_jaso[batch_index][sequence_index])):\n",
    "                char_index = batches_jaso[batch_index][sequence_index][word_index]\n",
    "                if char_index != 0.0:\n",
    "                    batches_jaso_input[batch_index][sequence_index][word_index][char_index] = 1\n",
    "    batches_jaso_input = np.reshape(batches_jaso_input, [len(batches), sequence_length, word_max_length, char_dim, 1])\n",
    "\n",
    "    batches_tag_onehot = []\n",
    "    for _, batch in enumerate(batches_tag):\n",
    "        indices = [0 for i in range(len(batch))]\n",
    "        for idx, tag in enumerate(batch):\n",
    "            if tag in dictionary_tag:\n",
    "                indices[idx] = dictionary_tag[tag]\n",
    "            else:\n",
    "                indices[idx] = 0\n",
    "        present_length = len(indices)\n",
    "        batch_tag_onehot = []\n",
    "        for i in range(present_length):\n",
    "            p = indices[i] \n",
    "            onehot = np.zeros(100, np.float32)\n",
    "            onehot[p] = 1.0\n",
    "            batch_tag_onehot.append(onehot)\n",
    "        batches_tag_onehot.append(batch_tag_onehot)\n",
    "\n",
    "    for i in range(len(batches_tag_onehot)):\n",
    "        present_length = len(batches_tag_onehot[i])\n",
    "        for _ in range(present_length, sequence_length):\n",
    "            batches_tag_onehot[i].append(PAD_VECTOR)\n",
    "\n",
    "    return np.transpose(np.asarray(batches_word_vector), [1,0,2]), np.transpose(batches_jaso_input, [1,0,2,3,4]), np.transpose(np.asarray(batches_tag_onehot), [1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs,scope,scale=True,layer_norm=True,epsilon = 1e-5):\n",
    "    \n",
    "    if layer_norm == True:\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            if scale == False:\n",
    "                scale = tf.ones([inputs.get_shape()[1]],tf.float32)\n",
    "            else:\n",
    "                scale = tf.get_variable(\"scale\", shape=[inputs.get_shape()[1]],\n",
    "                        initializer=tf.ones_initializer())\n",
    "        \n",
    "        \n",
    "\n",
    "        mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n",
    "        \n",
    "        LN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean))\n",
    "        \n",
    "        return LN\n",
    "    else:\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_rel(H_rel_head, H_rel_dep, prediction_head, w, u, b, seq_len, scope):\n",
    "    \n",
    "    batch_size = tf.shape(H_rel_head)[0]\n",
    "    b_hidden_lists = tf.TensorArray(size=batch_size,dtype=tf.float32)\n",
    "    j=0\n",
    "    def cond(i,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden_lists):\n",
    "        h_rel_head = H_rel_head[j][prediction_head[j][i]]\n",
    "        h_rel_dep = H_rel_dep[j][i]\n",
    "        h_rel_head = tf.reshape(h_rel_head, [rel_size, 1])\n",
    "        h_rel_dep = tf.reshape(h_rel_dep, [rel_size, 1])\n",
    "        s = tf.matmul(tf.matmul(tf.transpose(h_rel_head, [1, 0]), u), h_rel_dep) + tf.matmul(w, tf.concat([h_rel_dep, h_rel_head], 0)) + b\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,s)\n",
    "        return i+1,hidden_lists\n",
    "    \n",
    "    def b_cond(j, b_hidden_lists):\n",
    "        return j < batch_size\n",
    "    \n",
    "    def b_body(j, b_hidden_lists):\n",
    "        hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "        i = 0\n",
    "        _,hidden_lists = tf.while_loop(cond, body, [i, hidden_lists], swap_memory=True, name=\"get_score_rel_inner\")\n",
    "        hidden_lists = hidden_lists.stack()\n",
    "        b_hidden_lists = b_hidden_lists.write(j,hidden_lists)\n",
    "        return j+1, b_hidden_lists\n",
    "    \n",
    "    _, b_hidden_lists = tf.while_loop(b_cond, b_body, [j,b_hidden_lists], swap_memory=True, name=\"get_score_rel_outter\")\n",
    "    return b_hidden_lists.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_batch(x, word_max_length, char_dim, seq_len, scope):\n",
    "    filter_size = [3,5,7,9]\n",
    "    num_filters = 25\n",
    "    conved_layers = []\n",
    "    for f in filter_size:\n",
    "        Y = tf.layers.conv2d(x, filters=num_filters, kernel_size=[f, char_dim], activation=tf.nn.relu, padding= 'VALID',\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        Y = tf.layers.max_pooling2d(Y, pool_size=[word_max_length - f + 1, 1], strides=2, padding='VALID')\n",
    "        conved_layers.append(Y)\n",
    "    concat_chars = tf.concat(conved_layers,3)\n",
    "    character_embd = tf.reshape(concat_chars,[-1, char_vec_dim])\n",
    "    return character_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_cell(x, hidden, scope):\n",
    "    z = tf.sigmoid(layer_norm( tf.matmul(x,w_forward[0]) + tf.matmul(hidden,u_forward[0]) , scope+\"_z\")+ b_forward[0])\n",
    "    r = tf.sigmoid(layer_norm( tf.matmul(x,w_forward[1]) + tf.matmul(hidden,u_forward[1]) , scope+\"_r\")+ b_forward[1])\n",
    "    h_ = tf.tanh(layer_norm( tf.matmul(x,w_forward[2]) + tf.multiply(r,tf.matmul(hidden,u_forward[2])),scope+\"_h\") + b_forward[2])\n",
    "    hidden = tf.multiply(z,h_) + tf.multiply((1-z),hidden) \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer_1\"):\n",
    "    train_inputs = list()\n",
    "    for X_VEC, X_TAG, X_CHAR in zip(train_inputs_vec, train_inputs_tag, train_inputs_char):\n",
    "        cnn_embd = conv2d_batch(X_CHAR, word_max_length, char_dim, sequence_length, \"conv2d\")\n",
    "        word_embd = tf.add(cnn_embd, X_VEC)\n",
    "        X_Concat = tf.concat([word_embd, X_TAG], 1)\n",
    "        train_inputs.append(X_Concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer_2\"):\n",
    "    outputs = list()\n",
    "    f_output = tf.Variable(tf.zeros([batch_size,hidden_size],dtype=tf.float32),name='f_output',trainable=False)\n",
    "    b_output = tf.Variable(tf.zeros([batch_size,hidden_size],dtype=tf.float32),name='b_output',trainable=False)\n",
    "    for input in (train_inputs):\n",
    "        f_output = gru_cell(input, f_output, \"Forward\")\n",
    "        b_output = gru_cell(tf.reverse(input, [0]), b_output, \"Backward\")\n",
    "        outputs.append(tf.layers.dropout(inputs=tf.add(f_output, b_output), rate=dropout_rate, training=training))\n",
    "    outputs = tf.transpose(outputs, perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer_3\"):\n",
    "    W_arc_dep = tf.tile(w_arc_dep, multiples=[batch_size, 1])\n",
    "    W_arc_dep = tf.reshape(W_arc_dep, [batch_size, hidden_size, arc_size])\n",
    "    W_arc_head = tf.tile(w_arc_head, multiples=[batch_size, 1])\n",
    "    W_arc_head = tf.reshape(W_arc_head, [batch_size, hidden_size, arc_size])\n",
    "    W_rel_dep = tf.tile(w_rel_dep, multiples=[batch_size, 1])\n",
    "    W_rel_dep = tf.reshape(W_rel_dep, [batch_size, hidden_size, rel_size])\n",
    "    W_rel_head = tf.tile(w_rel_head, multiples=[batch_size, 1])\n",
    "    W_rel_head = tf.reshape(W_rel_head, [batch_size, hidden_size, rel_size])\n",
    "    B_arc_dep = tf.tile(b_arc_dep, multiples=[batch_size])\n",
    "    B_arc_dep = tf.reshape(B_arc_dep, [batch_size, arc_size, 1])\n",
    "    B_arc_head = tf.tile(b_arc_head, multiples=[batch_size])\n",
    "    B_arc_head = tf.reshape(B_arc_head, [batch_size, arc_size, 1])\n",
    "    B_rel_dep = tf.tile(b_rel_dep, multiples=[batch_size])\n",
    "    B_rel_dep = tf.reshape(B_rel_dep, [batch_size, rel_size, 1])\n",
    "    B_rel_head = tf.tile(b_rel_head, multiples=[batch_size])\n",
    "    B_rel_head = tf.reshape(B_rel_head, [batch_size, rel_size, 1])\n",
    "    H_arc_dep = tf.nn.relu(tf.matmul(outputs, W_arc_dep) + b_arc_dep)\n",
    "    H_arc_dep = tf.layers.dropout(inputs=H_arc_dep, rate=dropout_rate, training=training)\n",
    "    H_arc_head = tf.nn.relu(tf.matmul(outputs, W_arc_head) + b_arc_head)\n",
    "    H_arc_head = tf.layers.dropout(inputs=H_arc_head, rate=dropout_rate, training=training)\n",
    "    H_rel_dep = tf.nn.relu(tf.matmul(outputs, W_rel_dep) + b_rel_dep)\n",
    "    H_rel_dep = tf.layers.dropout(inputs=H_rel_dep, rate=dropout_rate, training=training)\n",
    "    H_rel_head = tf.nn.relu(tf.matmul(outputs, W_rel_head) + b_rel_head)\n",
    "    H_rel_head = tf.layers.dropout(inputs=H_rel_head, rate=dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_arc = tf.tile(w_arc, [batch_size, 1])\n",
    "W_arc = tf.reshape(W_arc, [batch_size, arc_size, arc_size])\n",
    "B_arc = tf.tile(b_arc, [batch_size, 1])\n",
    "B_arc = tf.reshape(B_arc, [batch_size, arc_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer_4\"):\n",
    "    arc_score = tf.matmul(tf.matmul(H_arc_head, W_arc), tf.transpose(H_arc_dep, perm=[0,2,1])) + tf.matmul(H_arc_head, B_arc)\n",
    "    arc_score = tf.layers.dropout(inputs=arc_score, rate=dropout_rate, training=training)\n",
    "    prediction_head = tf.reshape(tf.argmax(arc_score, 2), [batch_size, sequence_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer_5\"):\n",
    "    rel_score = get_score_rel(H_rel_head, H_rel_dep, prediction_head, w_rel, u_rel, b_rel, sequence_length, \"rel_score\")\n",
    "    rel_score = tf.layers.dropout(inputs=rel_score, rate=dropout_rate, training=training)\n",
    "    rel_score = tf.reshape(rel_score, [batch_size, sequence_length, label_dim])\n",
    "    prediction_relation = tf.reshape(tf.argmax(rel_score, 2), [batch_size, sequence_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_enable_10000.ckpt\n"
     ]
    }
   ],
   "source": [
    "save_file = \"./model_enable_10000.ckpt\"\n",
    "param_list = [w_forward, u_forward, b_forward, w_backward, u_backward, b_backward,\\\n",
    "              w_arc_dep, w_arc_head, w_rel_dep, w_rel_head,\\\n",
    "              b_arc_head, b_arc_dep, b_rel_dep, b_rel_head,\\\n",
    "              w_arc, b_arc, w_rel, u_rel, b_rel]\n",
    "batch_generator = mini_batch(batches_morphene, batches_arc, batches_relation)\n",
    "saver = tf.train.Saver(param_list)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, save_file)\n",
    "    X, Y_arc, Y_rel = batch_generator.next_batch(batch_size)\n",
    "    X_vec, X_char, X_tag = split_data(X)\n",
    "    feed_dict = dict()\n",
    "    feed_dict[training] = True\n",
    "    get_arc = list()\n",
    "    get_rel = list()\n",
    "    for ui,(x_vec, x_char, x_tag, y_arc, y_rel) in enumerate(zip(X_vec, X_char, X_tag, Y_arc, Y_rel)):\n",
    "        feed_dict[train_inputs_vec[ui]]=x_vec\n",
    "        feed_dict[train_inputs_char[ui]]=x_char\n",
    "        feed_dict[train_inputs_tag[ui]]=x_tag\n",
    "        get_arc.append(y_arc)\n",
    "        get_rel.append(y_rel)\n",
    "    get_arc = np.transpose(np.asarray(get_arc), axes=[1, 0, 2])\n",
    "    get_rel = np.transpose(np.asarray(get_rel), axes=[1, 0, 2])\n",
    "    feed_dict[train_labels_arc] = get_arc\n",
    "    feed_dict[train_labels_rel] = get_rel\n",
    "    score_arc, score_rel = sess.run([arc_score, rel_score], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree = enjoy_tree.enjoy_tree(X[0], score_arc[0], score_rel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.insert_node(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree.connect_all_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['그/MM', '다음/NNG', '에/JKB', '는/JX', '정말/NNG', '이/VCP', '지/EC', '한/MM', '자/NNB', '도/JX', '더/MAG', '쓰/VV', 'ᆯ/ETM', '말/NNG', '이/JKS', '없/VA', '었/EP', '다/EF', './SF'])\n"
     ]
    }
   ],
   "source": [
    "print(test_tree._graph.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = test_tree.get_mst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = {}\n",
    "for i in range(len(test_tree._nodenames)):\n",
    "    sentence_index[test_tree._nodenames[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_rel = np.argmax(score_rel[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'정말/NNG'-> '다음/NNG', 관계는 NNG\n",
      "'정말/NNG'-> '이/VCP', 관계는 VP\n",
      "'다음/NNG'-> '에/JKB', 관계는 VP\n",
      "'이/VCP'-> '는/JX', 관계는 NNG\n",
      "'이/VCP'-> '정말/NNG', 관계는 VP\n",
      "'이/VCP'-> '지/EC', 관계는 EF\n",
      "'이/VCP'-> '한/MM', 관계는 EF\n",
      "'이/VCP'-> '도/JX', 관계는 VP\n",
      "'이/VCP'-> '더/MAG', 관계는 NNG\n",
      "'이/VCP'-> '그/MM', 관계는 VP\n",
      "'그/MM'-> '자/NNB', 관계는 EF\n",
      "'그/MM'-> '쓰/VV', 관계는 EP\n",
      "'그/MM'-> '말/NNG', 관계는 EP\n",
      "'는/JX'-> 'ᆯ/ETM', 관계는 EP\n",
      "'지/EC'-> '이/JKS', 관계는 EP\n",
      "'에/JKB'-> '었/EP', 관계는 EP\n",
      "'에/JKB'-> '다/EF', 관계는 EP\n",
      "'다/EF'-> './SF', 관계는 UNK\n",
      "root -> 없/VA\n"
     ]
    }
   ],
   "source": [
    "for k in tree.keys():\n",
    "    for d in tree[k].keys():\n",
    "        print(\"\\'{0}\\'-> \\'{1}\\', 관계는 {2}\".format(k, d, dictionary_rel[score_rel[sentence_index[d]]]))\n",
    "\n",
    "print(\"root -> {0}\".format(test_tree._nodenames[test_tree._root]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-tf18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
